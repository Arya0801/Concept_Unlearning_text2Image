# ðŸš€ Concept Unlearning in Text-to-Image and Text-to-Video Diffusion Models

---

## Project Overview

This project tackles the critical issue of **unintentionally memorized copyrighted or sensitive content** in large-scale generative AI models, specifically text-to-image and text-to-video diffusion models. Retraining these models is computationally impractical due to massive computational costs. Our work proposes and implements **efficient unlearning methods** that eliminate targeted content without needing full retraining, ensuring ethical compliance while preserving general generative capabilities.

---

## Motivation & Problem Statement

Diffusion models' reliance on extensive datasets often leads to unintended replication of copyrighted material, private portraits, or unsafe content in generated outputs. This raises significant privacy, safety, and intellectual property concerns. Traditional solutions like dataset filtering or model retraining are computationally expensive and infeasible for pre-trained models. This highlights the crucial need for efficient methods to unlearn specific concepts from these models.

---

## Objectives

The primary objectives of this project were:
* To investigate and implement a **concept-unlearning technique** for diffusion models, particularly focusing on text-to-image and text-to-video domains.
* To develop an efficient unlearning approach that leverages the **shared architecture** between text-to-image and text-to-video models.
* To achieve concept unlearning through **minimal modifications to the text encoder** using few-shot learning techniques.
* To evaluate the unlearning performance using metrics such as the **CLIP score** while preserving the overall quality of generated content.

---

## Methodology

Our methodology focuses on efficiently erasing specific concepts by **fine-tuning the text encoder of Stable Diffusion** while freezing other components, including the U-Net and the VAE. This approach ensures better transferability and minimizes degradation in the model's overall generative capabilities.

### Text-to-Image Unlearning (Stable Diffusion v1.4)
We selected **Stable Diffusion v1.4** for text-to-image tasks. The **text encoder (CLIP)** served as the primary target for unlearning specific concepts. Prior work indicates that modifying the text encoder is more effective for concept erasure than altering the U-Net. The loss function was adapted from textual inversion, reversed to penalize alignment between text and images for undesired concepts.

To strengthen concept forgetting, **Attention Resteering** from Zhang et al. (2023) was also applied. This technique minimizes attention activations linked to the target concept within the U-Net's cross-attention layers, modifying intermediate attention maps to precisely suppress influence without token blacklisting or naive fine-tuning. This process is efficient (approximately 30 seconds).

### Text-to-Video Unlearning (SVD)
For text-to-video tasks, the **SVD (Stable Video Diffusion)** model was employed. It leverages its ability to synthesize coherent video frames from a sequence of images. The images generated by the unlearned Stable Diffusion model were used as input, ensuring that concept erasure was consistently applied across both modalities. This two-stage process was computationally efficient and ensured seamless unlearning transfer to videos.

### Concept Unlearning Process
The core concept unlearning involved lightweight modifications to the text encoder (specifically, the CLIP model) to reduce its semantic alignment with a designated concept. Parameter updates were performed to weaken the representation of the target concept in the encoded text prompt. A **few-shot learning strategy** was adopted, using a small set of images (four generated images associated with the target concept) to guide the unlearning process. This method demonstrated more consistent reductions in text-image alignment compared to zero-shot unlearning.

To complement encoder-level unlearning, the attention re-steering method introduced by Zhang et al. (2023) was adopted. This strategy directly modifies the model's internal attention maps by penalizing activations linked to the target concept within the cross-attention layers of the U-Net.

Three integration strategies were explored to assess how unlearning effects can be combined across different components:
1.  **Parallel Training and Encoder Transfer**: Two Stable Diffusion instances were trained independently (one with few-shot encoder unlearning, the other using LoRA-based attention re-steering), then the text encoder from the first was substituted into the second.
2.  **Sequential Attention Re-steering Followed by Encoder Unlearning**: Attention maps were first suppressed using LoRA, then the updated model was subjected to few-shot encoder tuning.
3.  **Sequential Encoder Unlearning Followed by Attention Re-steering**: Encoder tuning was performed prior to spatial attention suppression.

These strategies demonstrated varying effectiveness, highlighting the complementary nature of encoder and attention-level interventions.

---

## Evaluation Metrics

Evaluation relied on the **CLIP score**, which measures the alignment between textual prompts and visual outputs. This process involves:
* Passing the generated image through the CLIP image encoder to produce a feature vector ($V\_{image}$).
* Processing the reference text prompt through the CLIP text encoder to produce a corresponding feature vector ($V\_{text}$).
* Normalizing both feature vectors to have a unit L2-norm: $\\hat{V}*{image} = V*{image} / |V\_{image}|$, $\\hat{V}*{text} = V*{text} / |V\_{text}|$.
* Computing similarity as the cosine of the angle between these normalized vectors:
    $$
    \text{Similarity} = \hat{V}_{image} \cdot \hat{V}_{text} = \sum_i \hat{V}_{image,i} \cdot \hat{V}_{text,i}
    $$
    The score ranges from -1 (complete dissimilarity) to +1 (perfect match).

While CLIP provided quantitative assessment, **human evaluation** was also used to verify the erasure's effectiveness and the preservation of model quality. CLIP's latent space was analyzed using t-SNE to ensure cluster integrity for similar concepts despite encoder adjustments.

---

## Results

The unlearning process successfully produced images and videos with noticeably reduced text-to-image alignment for the targeted concepts.

* **Images**: For instance, when prompted with "the Eiffel Tower," generated outputs lacked discernible features resembling the iconic structure. Similar results were observed for the Taj Mahal and Burj Khalifa.
    * For "the Eiffel Tower," the average CLIP score dropped by approximately **62%**, while scores for unrelated prompts (e.g., "A photo of Red Fort" or "A photo of Statue of Liberty") remained largely unaffected.
* **Videos**: The unlearned model successfully extended this behavior to dynamic content, with videos synthesized using the SVD model demonstrating smooth transitions and temporal consistency without reintroducing the erased concepts.

---

## Tools & Frameworks

The project was implemented using:
* **Programming Language**: Python 3
* **Key Libraries**: PyTorch, Hugging Face Diffusers, NumPy, SciPy, Matplotlib, and tqdm
* **Hardware**: NVIDIA GPUs (RTX 4060 for text-to-image, RTX 3040 for SVD) with CUDA support
* **Platform**: Windows Subsystem for Linux (WSL)
* **Models**: Stable Diffusion v1.4 (text-to-image), SVD (text-to-video), CLIP (text encoder)
* **Evaluation Metrics**: CLIP Score

---

## Installation & Setup

1.  **Clone the repository:**
    ```bash
    git clone [https://github.com/Arya0801/Concept_Unlearning_text2Image.git](https://github.com/Arya0801/Concept_Unlearning_text2Image.git)
    cd Concept_Unlearning_text2Image
    ```
2.  **Set up WSL (if not already done):** Ensure you have WSL 2 installed and configured, particularly with sufficient memory allocation in `.wslconfig` (e.g., `memory=4GB`, `swap=2GB` in `C:\Users\YourWindowsUsername\.wslconfig`).
3.  **Create a Python virtual environment:**
    ```bash
    python3 -m venv venv
    source venv/bin/activate
    ```
4.  **Install necessary libraries:**
    ```bash
    pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu118](https://download.pytorch.org/whl/cu118) # Or appropriate CUDA version
    pip install diffusers transformers accelerate numpy scipy matplotlib tqdm clip-score
    ```

---

## Usage

(Based on the report, this section would detail how to run your unlearning scripts. This is a placeholder as the exact scripts/commands are not in the report content. You will need to fill this in with actual commands.)

1.  **Prepare your configuration:** Define the target concepts you wish to unlearn and specify any relevant paths for data.
2.  **Run the text-to-image unlearning script:**
    ```bash
    python scripts/unlearn_image_model.py --config configs/eiffel_tower_unlearn.yaml
    ```
3.  **Run the text-to-video generation script:**
    ```bash
    python scripts/generate_video_from_unlearned_images.py --input_images_path output/unlearned_images --prompt "a girl dancing in front of Taj Mahal"
    ```
4.  **Evaluate results:**
    ```bash
    python scripts/evaluate_clip_score.py --generated_images output/images --prompts "your prompt here"
    ```


---

## License

This project is licensed under the MIT License.
